{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Neural SEED.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Sequence Distance Embeddings\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gcorso/neural_seed/blob/master/tutorial/Neural_SEED.ipynb)\n",
    "\n",
    "The improvement of data-dependent heuristics and representation for biological sequences is a critical requirement to fully exploit the recent technological and scientific advancements for human microbiome analysis. This notebook presents Neural Sequence Distance Embeddings (Neural SEED), a novel framework to embed biological sequences in geometric vector spaces that unifies recently proposed approaches. We then demonstrate its capacity by presenting different ways it can be applied to the tasks of edit distance approximation, closest string retrieval, hierarchical clustering and multiple sequence alignment. In particular, the hyperbolic space is shown to be a key component to embed biological sequences and obtain competitive heuristics. Benchmarked with common bioinformatics and machine learning baselines, the proposed approaches displayed significant accuracy and/or runtime improvements on real-world datasets formed by sequences from samples of the human microbiome."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Cover](https://raw.githubusercontent.com/gcorso/neural_seed/master/tutorial/cover.png)\n",
    "\n",
    "Figure 1: On the left, a diagram of the Neural SEED underlying idea. On the right, an example of the hierarchical clustering produced on the Poincar√® disk from the P53 tumour protein from 30 different organisms.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction and Motivation\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Dysfunctions of the human microbiome (Morgan & Huttenhower, 2012) have been linked to many serious diseases ranging from diabetes and antibiotic resistance to inflammatory bowel disease and its usage as a biomarker for the diagnosis and as a target for interventions is a very active area of research. Thanks to the advances in sequencing technologies, modern analysis relies on sequence reads that can be generated relatively quickly. However, to fully exploit the potential of these advances for personalised medicine, the computational methods used in the analysis have to significantly improve in terms of speed and accuracy.\n",
    "\n",
    "![Classical microbiome analysis](https://raw.githubusercontent.com/gcorso/neural_seed/master/tutorial/microbiome_analysis.png)\n",
    "\n",
    "Figure 2: Traditional approach to the analysis of the 16S rRNA sequences from the microbiome. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem\n",
    "\n",
    "While the number of available biological sequences has been growing exponentially over the past decades, most of the problems related to string matching have not been addressed by the recent advances in machine learning. Classical algorithms are data-independent and, therefore, cannot exploit the low-dimensional manifold assumption that characterises real-world data. Exploiting the available data to produce data-dependent heuristics and representations would greatly speed up large-scale analyses that are critical to microbiome analysis and other biological research. \n",
    "\n",
    "Unlike most tasks in computer vision and NLP, string matching problems are typically formulated as combinatorial optimisation problems. These discrete formulations do not fit well with the current deep learning approaches causing these problems to be left mostly unexplored by the community. Current supervised learning methods also suffer from the lack of labels that characterises many downstream applications with biological sequences. Instead, common self-supervised learning approaches, very successful in NLP, are less effective in the biological context where relations tend to be per-sequence rather than per-token (McDermott et al. 2021)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Neural Sequence Distance Embedding\n",
    "\n",
    "In this notebook, we present Neural Sequence Distance Embeddings (Neural SEED), a general framework to produce representations for biological sequences where the distance in the embedding space is correlated to the evolutionary distance between sequences. This control over the geometric interpretation of the representation space enables the use of geometrical data processing tools for the analysis of the spectrum of sequences.\n",
    "\n",
    "![Classical microbiome analysis](https://raw.githubusercontent.com/gcorso/neural_seed/master/tutorial/edit_diagram.PNG)\n",
    "\n",
    "Figure 3: The key idea of Neural SEED is to learn an encoder function that preserves distances between the sequence and vector space.\n",
    "\n",
    "\n",
    "Examining the task of embedding sequences to preserve the edit distance reveals the importance of data-dependent approaches and of using a geometry that well matches the underlying distribution in the data analysed. For biological datasets, that have an implicit hierarchical structure given by evolution, the hyperbolic space provides significant improvement.\n",
    "\n",
    "We show the potential of the framework by analysing three fundamental tasks in bioinformatics: closest string retrieval, hierarchical clustering and multiple sequence alignment. For all tasks, relatively simple unsupervised approaches using Neural SEED encoder significantly outperform data-independent heuristics in terms of accuracy and/or runtime. In the paper (preprint will be available soon) and the [complete repository](https://github.com/gcorso/neural_seed) we also present more complex geometrical approaches to hierarchical clustering and multiple sequence alignment.\n",
    "\n",
    "\n",
    "## 2. Analysis\n",
    "\n",
    "To make the code in the notebook of a reasonable size we make use of some utils in the [official repository](https://github.com/gcorso/neural_seed) for the research project. The code in the notebook is our best effort to convey the promising application of hyperbolic geometry to this novel research direction and how `geomstats` helps to achieve it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install and import the required packages. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip3 install geomstats\n",
    "!apt install clustalw\n",
    "!pip install biopython\n",
    "!pip install python-Levenshtein\n",
    "!pip install Cython\n",
    "!pip install networkx\n",
    "!pip install tqdm\n",
    "!pip install gdown\n",
    "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!git clone https://github.com/gcorso/neural_seed.git\n",
    "import os\n",
    "os.chdir(\"neural_seed\")\n",
    "!cd hierarchical_clustering/relaxed/mst; python setup.py build_ext --inplace; cd ../unionfind; python setup.py build_ext --inplace; cd ..; cd ..; cd ..;\n",
    "os.environ['GEOMSTATS_BACKEND'] = 'pytorch'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from geomstats.geometry.poincare_ball import PoincareBall\n",
    "\n",
    "from edit_distance.train import load_edit_distance_dataset\n",
    "from util.data_handling.data_loader import get_dataloaders\n",
    "from util.ml_and_math.loss_functions import AverageMeter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset description\n",
    "\n",
    "As microbiome analysis is one of the most critical applications where the methods presented could be applied, we chose to use a dataset containing a portion of the 16S rRNA gene widely used in the biological literature to analyse microbiome diversity. Qiita (Clemente et al. 2015) contains more than 6M sequences of up to 152 bp that cover the V4 hyper-variable region collected from skin, saliva and faeces samples of uncontacted Amerindians. The full dataset can be found on the [European Nucleotide Archive](https://www.ebi.ac.uk/ena/browser/text-search?query=ERP008799), but, in this notebook, we will only use a subset of a few tens of thousands that have been preprocessed and labelled with pairwise distances. We also provide results on the RT988 dataset (Zheng et al. 2019), another dataset of 16S rRNA that contains slightly longer sequences (up to 465 bp)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!gdown --id 1yZTOYrnYdW9qRrwHSO5eRc8rYIPEVtY2 # for edit distance approximation\n",
    "!gdown --id 1hQSHR-oeuS9bDVE6ABHS0SoI4xk3zPnB # for closest string retrieval\n",
    "!gdown --id 1ukvUI6gUTbcBZEzTVDpskrX8e6EHqVQg # for hierarchical clustering"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Edit distance approximation\n",
    "\n",
    "**Edit distance**  The task of finding the distance or similarity between two strings and the related task of global alignment lies at the foundation of bioinformatics. Due to the resemblance with the biological mutation process, the edit distance and its variants are typically used between sequences. Given two string $s_1$ and $s_2$, their edit distance $ED(s_1, s_2)$ is defined as the minimum number of insertions, deletions or substitutions needed to transform $s_1$ in $s_2$. We always deal with the classical edit distance where the same weight is given to every operation, however, all the approaches developed can be applied to any distance function of choice. \n",
    "\n",
    "**Task and loss function** As represented in Figure 3, the task is to learn a encoding function $f$ such that given any pair of sequences from the domain of interest $s_1$ and $s_2$:\n",
    "\\begin{equation}ED(s_1, s_2) \\approx n \\; d(f(s_1), f(s_2)) \\end{equation}\n",
    "\n",
    "where $n$ is the maximum sequence length and $d$ is a distance function over the vector space. In practice this is enforced in the model by minimising the mean squared error between the actual and the predicted edit distance. To make the results more interpretable and comparable across different datasets, we report results using \\% RMSE defined as:\n",
    "\\begin{equation}\n",
    "\\text{% RMSE}(f, S) = \\frac{100}{n} \\, \\sqrt{L(f, S)} = \\frac{100}{n} \\, \\sqrt{\\sum_{s_1, s_2 \\in S} (ED(s_1, s_2) - n \\; d(f(s_1), f(s_2)))^2}\n",
    "\\end{equation}\n",
    "\n",
    "which can be interpreted as an approximate average error in the distance prediction as a percentage of the size of the sequences.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we only show the code to run a simple linear layer on the sequence which, in the hyperbolic space, already gives particularly good results. Later we will report results also for more complex models whose implementation can be found in the [Neural SEED repository](https://github.com/gcorso/neural_seed)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LinearEncoder(nn.Module):\n",
    "    \"\"\"  Linear model which simply flattens the sequence and applies a linear transformation. \"\"\"\n",
    "\n",
    "    def __init__(self, len_sequence, embedding_size, alphabet_size=4):\n",
    "        super(LinearEncoder, self).__init__()\n",
    "        self.encoder = nn.Linear(in_features=alphabet_size * len_sequence, \n",
    "                                 out_features=embedding_size)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # flatten sequence and apply layer\n",
    "        B = sequence.shape[0]\n",
    "        sequence = sequence.reshape(B, -1)\n",
    "        emb = self.encoder(sequence)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class PairEmbeddingDistance(nn.Module):\n",
    "    \"\"\" Wrapper model for a general encoder, computes pairwise distances and applies projections \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_model, embedding_size, scaling=False):\n",
    "        super(PairEmbeddingDistance, self).__init__()\n",
    "        self.hyperbolic_metric = PoincareBall(embedding_size).metric.dist\n",
    "        self.embedding_model = embedding_model\n",
    "        self.radius = nn.Parameter(torch.Tensor([1e-2]), requires_grad=True)\n",
    "        self.scaling = nn.Parameter(torch.Tensor([1.]), requires_grad=True)\n",
    "\n",
    "    def normalize_embeddings(self, embeddings):\n",
    "        \"\"\" Project embeddings to an hypersphere of a certain radius \"\"\"\n",
    "        min_scale = 1e-7\n",
    "        max_scale = 1 - 1e-3\n",
    "        return F.normalize(embeddings, p=2, dim=1) * self.radius.clamp_min(min_scale).clamp_max(max_scale)\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        \"\"\" Use embedding model and normalization to encode some sequences. \"\"\"\n",
    "        enc_sequence = self.embedding_model(sequence)\n",
    "        enc_sequence = self.normalize_embeddings(enc_sequence)\n",
    "        return enc_sequence\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # flatten couples\n",
    "        (B, _, N, _) = sequence.shape\n",
    "        sequence = sequence.reshape(2 * B, N, -1)\n",
    "\n",
    "        # encode sequences\n",
    "        enc_sequence = self.encode(sequence)\n",
    "\n",
    "        # compute distances\n",
    "        enc_sequence = enc_sequence.reshape(B, 2, -1)\n",
    "        distance = self.hyperbolic_metric(enc_sequence[:, 0], enc_sequence[:, 1])\n",
    "        distance = distance * self.scaling\n",
    "\n",
    "        return distance"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "General training and evaluation routines used to train the models:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def train(model, loader, optimizer, loss, device):\n",
    "    avg_loss = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    for sequences, labels in loader:\n",
    "        # move examples to right device\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # forward propagation\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sequences)\n",
    "\n",
    "        # loss and backpropagation\n",
    "        loss_train = loss(output, labels)\n",
    "        loss_train.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep track of average loss\n",
    "        avg_loss.update(loss_train.data.item(), sequences.shape[0])\n",
    "\n",
    "    return avg_loss.avg\n",
    "\n",
    "\n",
    "def test(model, loader, loss, device):\n",
    "    avg_loss = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    for sequences, labels in loader:\n",
    "        # move examples to right device\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # forward propagation and loss computation\n",
    "        output = model(sequences)\n",
    "        loss_val = loss(output, labels).data.item()\n",
    "        avg_loss.update(loss_val, sequences.shape[0])\n",
    "\n",
    "    return avg_loss.avg"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The linear model is trained on 7000 sequences (+700 of validation) and tested on 1500 different sequences: \n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}